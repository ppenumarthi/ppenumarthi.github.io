
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html><head><title>CSE512 Spring15</title>




  
  <meta http-equiv="content-type" content="text/html; charset=us-ascii" />
  <link href="css/main.css" rel="stylesheet" type="text/css" /></head><body>

<table align="center" bgcolor="white" border="0" cellpadding="0" cellspacing="0">
  <tbody>
    <tr>
      <td align="center" valign="top">
        <table border="0" cellpadding="0" cellspacing="0">
          <tbody>
            <tr>
              <td rowspan="1" height="77" width="140"><a href="#content"><img src="images/spacer.gif" alt="skip to page content" border="0" height="1" width="1" /></a><a href="http://www.cs.sunysb.edu/"><img src="images/CSdeptLogo.jpg" alt="SBU" border="0" height="120" hspace="10" width="120" /></a></td>
              <td align="center" height="45" width="788"><font face="Verdana" size="5"><b><br />
                Stony Brook University <br />
                Machine Learning <br />
                </b></font> <font face="Verdana" size="4"><span style="font-weight: bold;">CSE512 - Spring
                2015</span> </font></td>
            </tr>
          </tbody>
        </table>
      </td>
    </tr>
    <tr>
      <td><hr />
      </td>
    </tr>
    <tr>
      <td>
        <table summary="" border="0" cellpadding="0" cellspacing="0" width="56%">
          <tbody>
            <tr align="left" valign="top">
              <td height="4" width="166"><img src="images/spacer.gif" alt="" height="3" width="165" /></td>
              <td height="4" width="20"><img src="images/spacer.gif" alt="" height="1" width="20" /></td>
              <td height="4"><img src="images/spacer.gif" alt="" height="1" width="1" /></td>
            </tr>
            <tr>
              <td rowspan="2" align="left" bgcolor="#dddddd" valign="top" width="165">
                <table summary="" border="0" cellpadding="0" cellspacing="10" width="165">
                  <tbody>
                    <tr>
                      <td align="center" height="1"><img src="images/spacer.gif" alt="" height="1" width="1" /></td>
                    </tr>
                    <tr>
                      <td align="center" valign="top"><a href="index.htm"><img src="images/nav_home_off.gif" alt="Home" name="home" id="home" border="0" height="18" width="125" /></a></td>
                    </tr>
                    <tr>
                      <td align="center" valign="top"><a href="syllabus.htm"><img src="images/nav_syllabus.gif" alt="Syllabus" name="syllabus" id="syllabus" border="0" height="18" width="125" /></a></td>
                    </tr>
                    <tr>
                      <td align="center" valign="top"><a href="assignments.htm"><img src="images/nav_assignments_off.gif" alt="Assignments" name="assignments" id="assignments" border="0" height="18" width="125" /></a></td>
                    </tr>
                    <tr>
                      <td align="center" valign="top"><a href="policy.htm"><img src="images/nav_policy_off.gif" alt="Notes" name="notes" id="notes" border="0" height="18" width="125" /></a></td>
                    </tr>
                    <tr>
                      <td align="center" valign="top"><a href="resources.htm"><img src="images/nav_sources_off.gif" height="18" width="125" /></a></td>
                    </tr>
                  </tbody>
                </table>
              </td>
              <td align="left" height="16" valign="top" width="20"><img src="images/spacer.gif" alt="" height="16" width="20" /></td>
              <td align="left" height="16" valign="top"><img src="images/spacer.gif" alt="" height="16" width="1" /></td>
            </tr>
            <tr align="left" valign="top">
              <td height="300" width="20"><a name="content" id="content"><img src="images/spacer.gif" alt="" height="300" width="1" /></a></td>
              <td>
                <div id="subtext">
                <h2><font size="5"><b>Syllabus (download as <a href="ML-syllabus.pdf">pdf</a>)</b></font></h2>
                <br />


                <table summary="" border="0" cellpadding="2" cellspacing="3" width="740">
                  <tbody>
                    <tr bgcolor="#afafaf">
                      <th width="36"><h1>
                        <div style="padding: 1px; font-size: 16px; width: 36px; color: rgb(252, 252, 173);">
                        Date<br />
                        </div>
                        </h1>
                      </th>
                      <th width="687"><h1>
                        <div style="padding: 1px; font-size: 20px; width: 687px; color: rgb(252, 252, 173);">
                        Lectures and Readings<br />
                        </div>
                        </h1>
                      </th>
                    </tr>
                    <tr>
                      <td height="40" valign="baseline"><p>&nbsp;&nbsp;&nbsp;</p>

                        <p>1/27</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        <p>1/29</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        <p>2/3 <br />
</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>
                      </td>
                      <td><p>

                        </p><h1><strong><span style="color: rgb(255, 0, 0); font-weight: bold;">Introduction
                        to Machine Learning, Basics</span> (3 lectures)
                        </strong> </h1>

                        <h1><strong>Lecture 1: Intro to ML</strong>
                        <!-- [<a href="myslides/week1_1.pdf">slides</a>] --><br />
                        </h1>
                        <ul>
                          <li>What is ML? ML applications</li>
                          <li>Learning paradigms
                            <ul>
                              <li>Supervised learning (regression,
                                classification)</li>
                              <li>Unsupervised learning (density estimation,
                                clustering, dimensionality reduction)</li>
                            </ul>
                          </li>
                        </ul>

                        <p><span style="color: rgb(0, 0, 255); font-weight: bold;"><u>Readings</u>:</span>
                        </p>
                        <ul>
                          <li>Bishop 2.1, Appendix B</li>
                          <li>(Optional) Mitchell, Ch 1</li>
                          <li>(Optional) Murphy, 1.1, 1.2, 1.3.1</li>
                        </ul>

                        <br />
<p>

                        </p><p>

                        </p><p>

                        </p><p>

                        </p><p>

                        </p><div style="padding: 1px; width: 690px; background-color: rgb(252, 252, 173);">
                        <h1><strong>Recitation (Basics of Probability &amp;
                        Intro to Matlab)<br />
                        </strong></h1>
                        </div>

                        <p><br />
                        </p>

                        <h1><strong>Lecture 2: Learning
                        Distributions</strong>
                        <!-- [<a href="myslides/week1_1.pdf">slides</a>] --><br />
                        </h1>
                        <ul>
                          <li>Point estimation
                            <ul>
                              <li>Maximum Likelihood Estimation (MLE)</li>
                              <li>Bayesian learning</li>
                              <li>Maximum A Posterior (MAP) Estimation</li>
                            </ul>
                          </li>
                          <li>MLE vs. MAP</li>
                          <li>Gaussians </li>
                          <li>What is ML revisited</li>
                        </ul>

                        <p><span style="color: rgb(0, 0, 255); font-weight: bold;"><u>Readings</u>:</span>
                        </p>
                        <ul>
                          <li>Bishop: Sec 1.5, 2.2, 2.3 (up to 2.3.6) </li>
                          <li>(Additional Resource) <a href="http://www.autonlab.org/tutorials/prob18.pdf">Andrew
                            Moore's basic probability tutorial</a> </li>
                        </ul>
                        <br />
</td>
                    </tr>
                    <tr bgcolor="#e0e0e0">
                      <td height="2" valign="baseline"><br />
                  <br />
                  <br />
2/5<p>&nbsp;</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        

                        

                        

                        <p>&nbsp;<br />
                  <br />
                  <br />
2/10</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        <p><br />
</p>
                  <p>2/12</p>

                      </td>
                      <td><p>

                        </p><h1><strong><span style="color: rgb(255, 0, 0); font-weight: bold;">Linear
                        Models (Regression, Classification)</span> (3 lectures)
                        </strong> </h1>

                        <h1><strong>Lecture 3: Linear Regression </strong>
                        <!-- [<a href="myslides/week1_1.pdf">slides</a>] --><br />
                        </h1>
                        <ul>
                          <li>Linear Regression, [<a href="http://www.mste.uiuc.edu/users/exner/java.f/leastsquares/">Applet</a>]
                          </li>
                          <li>Regularized Least Squares, </li>
                          <li>Overfitting, </li>
                          <li>Bias-Variance Tradeoff, </li>
                        </ul>

                        <p><span style="color: rgb(0, 0, 255); font-weight: bold;"><u>Readings</u>:</span>
                        </p>
                        <ul>
                          <li>Bishop 1.1 to 1.4, </li>
                          <li>Bishop 3.1, 3.1.1, 3.1.4, 3.1.5, 3.2, 3.3, 3.3.1,
                            3.3.2 </li>
                          <li>(Additional Resource) <a href="http://www.autonlab.org/tutorials/introreg.html">Andrew
                            Moore&#8217;s Tutorial on regression</a> </li>
                          <li>(Optional) Hastie, Ch 7 </li>
                          <li>(Optional) Murphy, 1.4, Ch 7 </li>
                        </ul>
                        <br />


                        <h1><strong>Lecture 4: Naive Bayes</strong>
                        <!-- [<a href="myslides/week1_1.pdf">slides</a>] --><br />
                        </h1>
                        <ul>
                          <li>Bayes Optimal Classifier</li>
                          <li>Conditional Independence,</li>
                          <li>Naive Bayes, [<a href="http://www.cs.technion.ac.il/%7Erani/LocBoost/">Applet</a>]
                          </li>
                          <li>Gaussian Naive Bayes </li>
                        </ul>

                        <p><span style="color: rgb(0, 0, 255); font-weight: bold;"><u>Readings</u>:</span>
                        </p>
                        <ul>
                          <li>Bishop 1.3, 1.5, 3.2,</li>
                          <li><a href="http://www.cs.cmu.edu/%7Etom/mlbook/NBayesLogReg.pdf">Mitchell's
                            Chapter on Naive Bayes and Logistic Regression </a>
                            (Sect. 1 and 2)</li>
                          <li>(Optional) Murphy, 3.4</li>
                        </ul>
                        <br />


                        <h1><strong>Lecture 5: Logistic Regression</strong>
                        <!-- [<a href="myslides/week1_1.pdf">slides</a>] --><br />
                        </h1>
                        <ul>
                          <li>Generative v. Discriminative</li>
                          <li>Logistic Regression [<a href="http://www.cs.technion.ac.il/%7Erani/LocBoost/">Applet</a>]
                          </li>
                        </ul>

                        <p><span style="color: rgb(0, 0, 255); font-weight: bold;"><u>Readings</u>:</span>
                        </p>
                        <ul>
                          <li><a href="http://www.cs.cmu.edu/%7Etom/mlbook/NBayesLogReg.pdf">Mitchell's
                            Chapter on Naive Bayes and Logistic Regression </a>
                            (Sect. 1 and 2)</li>
                          <li>Bishop - 4.0, 4.2, 4.3, 4.4, 4.5</li>
                          <li>(Optional) Murphy, Ch 8</li>
                          <li>(Optional) <a href="http://www.cs.cmu.edu/%7Eguestrin/Class/10701-F07/readings/ng-jordan-2001.pdf">Ng
                            and Jordan's NIPS 2001 paper on Discriminative
                            versus Generative Learning</a></li>
                        </ul>
                      </td>
                    </tr>
                    <tr>
                      <td height="0" valign="top"><p>&nbsp;</p>

                        <p>2/17</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        <p>2/19</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        <p>2/24</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        

                        

                        

                        <p>&nbsp;<br />
                  <br />
                  <br />
2/26</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        
                  <p><br />
3/3</p>

                      </td>
                      <td><p>

                        </p><h1><strong><span style="color: rgb(255, 0, 0); font-weight: bold;">Non-linear
                        Models and Model Selection</span> (5 lectures)
                        </strong> </h1>

                        <h1><strong>Lecture 6: Decision Trees </strong>
                        <!-- [<a href="myslides/week1_1.pdf">slides</a>] --><br />
                        </h1>
                        <ul>
                          <li>Decision Trees [<a href="http://www.cs.ualberta.ca/%7Eaixplore/learning/DecisionTrees/Applet/DecisionTreeApplet.html">Applet</a>]
                          </li>
                          <li>Entropy, Information Gain </li>
                          <li>Overfitting, Pre-and Post-pruning </li>
                        </ul>

                        <p><span style="color: rgb(0, 0, 255); font-weight: bold;"><u>Readings</u>:</span>
                        </p>
                        <ul>
                          <li>(Bishop - 1.6) Information Theory</li>
                          <li>(Bishop - 14.4) Tree-based Models</li>
                          <li>(Recommended) <a href="http://en.wikipedia.org/wiki/Quantities_of_information">Quantities
                            of Information</a> Wikipedia entry</li>
                          <li>(Recommended) Nils Nilsson's ML book (Ch 6, all
                            sections): <a href="http://robotics.stanford.edu/%7Enilsson/MLBOOK.pdf">Decision
                            Trees</a></li>
                          <li>(Optional) Mitchell, Ch 3 </li>
                          <li>(Optional) Murphy, 16.2</li>
                        </ul>
                        <br />


                        <h1><strong>Lecture 7: Boosting </strong>
                        <!-- [<a href="myslides/week1_1.pdf">slides</a>] --><br />
                        </h1>
                        <ul>
                          <li>Combining weak classifiers</li>
                          <li>Adaboost algorithm [<a href="http://www.cse.ucsd.edu/%7Eyfreund/adaboost/">Adaboost
                            Applet</a>]</li>
                          <li>Comparison with logistic regression and
                          bagging</li>
                        </ul>

                        <p><span style="color: rgb(0, 0, 255); font-weight: bold;"><u>Readings</u>:</span>
                        </p>
                        <ul>
                          <li>(Bishop 14.3) Boosting</li>
                          <li>(Additional Resource) <a href="http://www.cs.princeton.edu/%7Eschapire/boost.html">Boosting
                            homepage</a></li>
                          <li>(Recommended) <a href="http://www.cs.cmu.edu/%7Eguestrin/Class/10701-F07/readings/boosting-schapire.ps">Schapire
                            Boosting Tutorial</a>, and its [<a href="http://videolectures.net/mlss05us_schapire_b">Video</a>].</li>
                          <li>(Optional) <a href="http://www-stat.stanford.edu/%7Ehastie/Papers/samme.pdf">Multi-class
                            AdaBoost</a> by Zhu, Rosset, Zou, and Hastie.</li>
                          <li>(Optional) Murphy, 16.4</li>
                        </ul>
                        <br />


                        <h1><strong>Lecture 8: Model Selection </strong>
                        <!-- [<a href="myslides/week1_1.pdf">slides</a>] --><br />
                        </h1>
                        <ul>
                          <li>Cross Validation, </li>
                          <li>Simple Model Selection, </li>
                          <li>Regularization, </li>
                          <li>Information Criteria (AIC, BIC, MDL)</li>
                        </ul>

                        <p><span style="color: rgb(0, 0, 255); font-weight: bold;"><u>Readings</u>:</span>
                        </p>
                        <ul>
                          <li>(Bishop 1.3) Model Selection / Cross
                          Validation</li>
                          <li>(Optional) (Hastie et al. 3.2, 3.3, 3.4) Model
                            selection and L1 regularization</li>
                          <li>(Optional) Ron Kohavi's <a href="http://robotics.stanford.edu/%7Eronnyk/accEst.ps">A
                            Study of Cross-Validation and Bootstrap for
                            Accuracy Estimation and Model Selection</a></li>
                          <li>(Additional Resource) <a href="http://www.mdl-research.org/">Minimum
                            Description Length website</a></li>
                        </ul>
                        <br />


                        <h1><strong>Lecture 9: Neural Networks </strong>
                        <!-- [<a href="myslides/week1_1.pdf">slides</a>] --><br />
                        </h1>
                        <ul>
                          <li>Neural Nets [<a href="http://neuron.eng.wayne.edu/bpFunctionApprox/bpFunctionApprox.html">Applet</a>]</li>
                          <li>Prediction &#8211; Forward-propagation</li>
                          <li>Training &#8211; Back-propagation</li>
                        </ul>

                        <p><span style="color: rgb(0, 0, 255); font-weight: bold;"><u>Readings</u>:</span>
                        </p>
                        <ul>
                          <li>(Bishop 5.1) Feed-forward Network Functions</li>
                          <li>(Bishop 5.2) Network Training</li>
                          <li>(Bishop 5.3) Error Back-propagation</li>
                          <li>(Additional Resource) [<a href="http://www.cs.cmu.edu/afs/cs/academic/class/15782-s04/">CMU
                            Course</a>] on Neural Nets</li>
                          <li>(Optional) Murphy, 16.5</li>
                        </ul>
                        <br />


                        <h1><strong>Lecture 10: Nonparametric Methods </strong>
                        <!-- [<a href="myslides/week1_1.pdf">slides</a>] --><br />
                        </h1>
                        <ul>
                          <li>Instance-based Learning [<a href="http://www.site.uottawa.ca/%7Egcaron/applets.htm">Applet</a>]
                          </li>
                          <li>Histogram, Kernel Density Estimation</li>
                          <li>K-NN Classifier</li>
                          <li>Kernel Regression</li>
                        </ul>

                        <p><span style="color: rgb(0, 0, 255); font-weight: bold;"><u>Readings</u>:</span>
                        </p>
                        <ul>
                          <li>(Bishop 2.5, 6.3) Nonparametric Methods</li>
                          <li>(Optional) Mitchell, Ch 8</li>
                          <li>(Recommended) <a href="http://www.autonlab.org/tutorials/mbl08.pdf">Andrew
                            Moore&#8217;s Tutorial on Instance-based
                          Learning</a></li>
                        </ul>
                      </td>
                    </tr>
                    <tr bgcolor="#e0e0e0">
                      <td height="0" valign="top"><p>&nbsp;</p>

                        <p>3/5</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        

                        <p>&nbsp;<br />
3/10</p>

                        <p>&nbsp;</p>
                      </td>
                      <td><p>

                        </p><h1><strong><span style="color: rgb(255, 0, 0); font-weight: bold;">Margin-based
                        Approaches</span> (2 lectures) </strong> </h1>

                        <h1><strong>Lecture 11: Support Vector Machines
                        </strong>
                        <!-- [<a href="myslides/week1_1.pdf">slides</a>] --><br />
                        </h1>
                        <ul>
                          <li>SVM Representation [<a href="http://www.csie.ntu.edu.tw/%7Ecjlin/libsvm/">LibSVM
                            Applet</a>] [<a href="http://svm.dcs.rhbnc.ac.uk/pagesnew/GPat.shtml">Another
                            SVM Applet</a>]</li>
                          <li>Maximum Margin Classifiers</li>
                          <li>Slack variables, Hinge loss</li>
                        </ul>

                        <p><span style="color: rgb(0, 0, 255); font-weight: bold;"><u>Readings</u>:</span>
                        </p>
                        <ul>
                          <li>(Bishop 7.1, 4.1.1, 4.1.2, Appendix E)</li>
                          <li><a href="http://select.cs.cmu.edu/class/10701-F09/readings/hearst98.pdf">Hearst
                            1998: High Level Presentation</a></li>
                          <li><a href="http://www.kernel-machines.org/papers/Burges98.ps.gz">Burges
                            1998: Detailed Tutorial</a></li>
                          <li>(Additional Resource) <a href="http://videolectures.net/mlss07_smola_intkmet/">Smola
                            video tutorial on SVM</a> (see Part 3)</li>
                          <li>(Additional Resource) <a href="http://videolectures.net/mlss07_scholkopf_intkmet/">Scholkopf
                            video tutorial on kernels</a></li>
                          <li>(Additional Resource) <a href="http://www.svms.org/">http://www.svms.org</a></li>
                          <li>(Optional) Murphy, 14.5</li>
                        </ul>
                        <br />


                        <h1><strong>Lecture 12: The Kernel Trick </strong>
                        <!-- [<a href="myslides/week1_1.pdf">slides</a>] --><br />
                        </h1>
                        <ul>
                          <li>Dual SVM</li>
                          <li>Kernel Trick</li>
                          <li>Comparison with Kernel regression and Logistic
                            Regression</li>
                        </ul>

                        <p><span style="color: rgb(0, 0, 255); font-weight: bold;"><u>Readings</u>:</span>
                        </p>
                        <ul>
                          <li>(Bishop 6.1, 6.2) Kernels </li>
                          <li>(Additional Resource) <a href="http://www.kernel-machines.org/">http://www.kernel-machines.org</a></li>
                          <li>(Optional) Murphy, 14.4</li>
                        </ul>
                      </td>
                    </tr>
                    <tr>
                      <td height="40" valign="baseline">
                  <p><br />
3/12</p>


                        

                        <p>&nbsp;<br />
3/17 3/19</p>

                        

                        

                        <p>&nbsp;<br />
                  <br />
3/24</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        <p>3/26</p>
                      </td>
                      <td><br />


                        <h1>
                        <div style="padding: 1px; width: 690px; background-color: rgb(252, 252, 173);">
                        <strong>Midterm Exam<br />
                        </strong></div>
                         <!-- [<a href="myslides/week1_1.pdf">slides</a>] --><br />
                        </h1>

                        <h1>
                        <div style="padding: 1px; width: 690px; background-color: rgb(252, 252, 173);">
                        <strong>NO CLASS (Spring Break)<br />
                        </strong></div>
                         <!-- [<a href="myslides/week1_1.pdf">slides</a>] --><br />
                        </h1>

                        <p>

                        </p><h1><strong><span style="color: rgb(255, 0, 0); font-weight: bold;">Learning
                        Theory</span> (2 lectures) </strong> </h1>

                        <h1><strong>Lecture 13: PAC Learning</strong>
                        <!-- [<a href="myslides/week1_1.pdf">slides</a>] --><br />
                        </h1>
                        <ul>
                          <li>PAC-learning [<a href="http://www.site.uottawa.ca/%7Egcaron/applets.htm">Applets</a>]
                          </li>
                          <li>Sample complexity</li>
                          <li>Haussler bound, Hoeffding's bound</li>
                        </ul>

                        <p><span style="color: rgb(0, 0, 255); font-weight: bold;"><u>Readings</u>:</span>
                        </p>
                        <ul>
                          <li><a href="http://learningtheory.org/articles/COLTSurveyArticle.ps">Goldman's
                            COLT survey, sections 1-3.1</a></li>
                          <li>(Recommended) Mitchell Ch 7</li>
                          <li>(Optional) <a href="http://hunch.net/%7Ejl/projects/prediction_bounds/tutorial/langford05a.pdf">John
                            Langford's tutorial on generalization
                          bounds</a></li>
                          <li>(Additional Resource) <a href="http://videolectures.net/mlss05us_langford_gb/">Langford
                            video tutorial on generalization bounds</a></li>
                          <li>(Additional Resource) <a href="http://videolectures.net/mlss04_taylor_slt/">John
                            Shawe-Taylor video tutorial on statistical learning
                            theory</a></li>
                          <li>(Additional Resource) <a href="http://www.learningtheory.org">http://www.learningtheory.org</a></li>
                        </ul>
                        <br />


                        <h1><strong>Lecture 14: VC Dimension</strong>
                        <!-- [<a href="myslides/week1_1.pdf">slides</a>] --><br />
                        </h1>
                        <ul>
                          <li>VC Dimension</li>
                          <li>Mistake Bounds</li>
                          <li>Midterm exam review</li>
                        </ul>

                        <p><span style="color: rgb(0, 0, 255); font-weight: bold;"><u>Readings</u>:</span>
                        </p>
                        <ul>
                          <li>(Recommended) Mitchell Ch 7</li>
                          <li>(Optional) Littlestone's original (excellent)
                            paper on the Mistake Bound model: <a href="http://www.iua.upf.es/activitats/semirec/NaturalLanguage/fulltext-2.pdf">Learning
                            Quickly When Irrelevant Attributes Abound: A New
                            Linear-Threshold Algorithm</a></li>
                        </ul>
                      </td>
                    </tr>
                    <tr bgcolor="#e0e0e0">
                      <td height="0" valign="top"><p>&nbsp;</p>

                        <p>3/31</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        

                        <p><br />
4/2</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        <p>4/7</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        <p><br />
</p>
                  <p>4/9</p>

                      </td>
                      <td><p>

                        </p><h1><strong><span style="color: rgb(255, 0, 0); font-weight: bold;">Structured
                        Models (Graphical Models and HMM)</span> (4 lectures)
                        </strong> </h1>

                        <h1><strong>Lecture 15: Bayesian Networks &#8211;
                        Representation </strong>
                        <!-- [<a href="myslides/week1_1.pdf">slides</a>] --><br />
                        </h1>
                        <ul>
                          <li>Bayes Nets [<a href="http://www.pmr.poli.usp.br/ltd/Software/javabayes/Home/">Applet:
                            Java Bayes</a>] [<a href="http://www.aispace.org/bayes/version5.1.6/bayes.jnlp">Another
                            Bayes net applet</a>]</li>
                          <li>Factorization of joint distribution</li>
                          <li>Local Markov Assumption</li>
                          <li>D-separation</li>
                          <li>Representation Theorem</li>
                        </ul>

                        <p><span style="color: rgb(0, 0, 255); font-weight: bold;"><u>Readings</u>:</span>
                        </p>
                        <ul>
                          <li>(Bishop 8.1, 8.2) Bayesian Networks</li>
                          <li><a href="http://www.cs.cmu.edu/%7Eaarti/Class/10701/readings/intro_gm.pdf">Intro
                            to Graphical Models</a> by K. Murphy</li>
                          <li>(Optional) Murphy, Ch 10</li>
                        </ul>
                        <br />


                        <h1><strong>Lecture 16: Bayesian Networks &#8211;
                        Inference </strong>
                        <!-- [<a href="myslides/week1_1.pdf">slides</a>] --><br />
                        </h1>
                        <ul>
                          <li>Marginalization</li>
                          <li>Variable Elimination</li>
                        </ul>

                        <p><span style="color: rgb(0, 0, 255); font-weight: bold;"><u>Readings</u>:</span>
                        </p>
                        <ul>
                          <li>(Bishop 8.4.1, 8.4.2) - Inference in Chain/Tree
                            Structures </li>
                          <li>(Optional) Murphy, 10.3</li>
                        </ul>
                        <br />


                        <h1><strong>Lecture 17: Bayesian Networks &#8211;
                        Structure Learning </strong>
                        <!-- [<a href="myslides/week1_1.pdf">slides</a>] --><br />
                        </h1>
                        <ul>
                          <li>Learning CPTs</li>
                          <li>Learning structure - Chow-Liu Algorithm</li>
                        </ul>

                        <p><span style="color: rgb(0, 0, 255); font-weight: bold;"><u>Readings</u>:</span>
                        </p>
                        <ul>
                          <li>(Additional resource) <a href="http://www.cis.upenn.edu/%7Etaskar/pubs/gms-srl07.pdf">Koller
                            et. al, Graphical Models in a Nutshell</a></li>
                          <li>(Optional) Murphy, 26.1-26.4</li>
                          <li>(Additional resource) <a href="ftp://ftp.research.microsoft.com/pub/tr/tr-95-06.pdf">Heckerman
                            BN Learning Tutorial</a></li>
                          <li>(Additional reading) <a href="http://www.cs.huji.ac.il/%7Enir/Papers/FrGG1.pdf">Tree-Augmented
                            Naive Bayes</a></li>
                        </ul>
                        <br />


                        <h1><strong>Lecture 18: Hidden Markov Models </strong>
                        <!-- [<a href="myslides/week1_1.pdf">slides</a>] --><br />
                        </h1>
                        <ul>
                          <li>HMM Representation</li>
                          <li>Forward Algorithm</li>
                          <li>Forward-Backward Algorithm</li>
                          <li>Viterbi Algorithm</li>
                          <li>Baum-Welch Algorithm</li>
                        </ul>

                        <p><span style="color: rgb(0, 0, 255); font-weight: bold;"><u>Readings</u>:</span>
                        </p>
                        <ul>
                          <li>(Bishop, Ch 13) </li>
                          <li><a href="http://www.cs.cmu.edu/%7Eaarti/Class/10701/readings/gentle_tut_HMM.pdf">HMM
                            and EM Tutorial</a></li>
                          <li>(Optional) <a href="http://select.cs.cmu.edu/class/10701-F09/readings/hmms-rabiner.pdf">Rabiner's
                            Detailed HMMs Tutorial</a></li>
                          <li>(Additional Resource) <a href="http://select.cs.cmu.edu/class/10701-F09/readings/zoubin-hmms.pdf">Ghahramani,
                            An introduction to HMMs and Bayesian
                          Networks</a></li>
                          <li>(Optional) Murphy, Ch 17</li>
                        </ul>
                      </td>
                    </tr>
                    <tr>
                      <td height="40" valign="baseline"><p>&nbsp;</p>

                        <p>4/14</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        <p>4/16</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        <p>&nbsp;</p>

                        

                        <p>&nbsp;</p>
                  <p>4/21</p>


                  
                      </td>

                      <td><p>

                        </p><h1><strong><span style="color: rgb(255, 0, 0); font-weight: bold;">Unsupervised
                       learning</span> (3 lectures)
                        </strong> </h1>

                        <h1><strong>Lecture 19: Clustering I</strong>
                        <!-- [<a href="myslides/week1_1.pdf">slides</a>] --><br />
                        </h1>
                        <ul>
                          <li>Hierarchical Clustering</li>
                          <li>Spectral Clustering [<a href="http://www.ml.uni-saarland.de/GraphDemo/DemoSpectralClustering.html">Demo</a>]</li>
                        </ul>

                        <p><span style="color: rgb(0, 0, 255); font-weight: bold;"><u>Readings</u>:</span>
                        </p>
                        <ul>
                          <li><a href="http://www.cs.cmu.edu/%7Eaarti/Class/10701/readings/Luxburg06_TR.pdf">Spectral
                            Clustering tutorial</a> by Ulrike von Luxburg</li>
                          <li>(Optional) Murphy, 25.4, 25.5</li>
                        </ul>
                        <br />


                        <h1><strong>Lecture 20: Clustering II</strong>
                        <!-- [<a href="myslides/week1_1.pdf">slides</a>] --><br />
                        </h1>
                        <ul>
                          <li>K-Means [<a href="http://home.dei.polimi.it/matteucc/Clustering/tutorial_html/AppletKM.html">Applet:
                            K-means</a>]</li>
                          <li>Gaussian Mixture Model [<a href="http://www.neurosci.aist.go.jp/%7Eakaho/MixtureEM.html">Applet:
                            Mixture of Gaussians</a>]</li>
                        </ul>

                        <p><span style="color: rgb(0, 0, 255); font-weight: bold;"><u>Readings</u>:</span>
                        </p>
                        <ul>
                          <li>(Bishop 9.1, 9.2) - K-means, Mixtures of
                          Gaussian</li>
                        </ul>
                        <br />


                        <h1><strong>Lecture 21: Expectation Maximization
                        </strong>
                        <!-- [<a href="myslides/week1_1.pdf">slides</a>] --><br />
                        </h1>
                        <ul>
                          <li>EM Algorithm</li>
                        </ul>

                        <p><span style="color: rgb(0, 0, 255); font-weight: bold;"><u>Readings</u>:</span>
                        </p>
                        <ul>
                          <li>(Bishop 9.3, 9.4) &#8211; EM</li>
                          <li><a href="ftp://ftp.cs.utoronto.ca/pub/radford/emk.pdf">Neal
                            and Hinton EM paper</a></li>
                          <li>(Optional) Murphy, Ch 11</li>
                        </ul>
                        <br />
                         </td>




                  
                    <tr bgcolor="#e0e0e0">
                      <td height="0" valign="top"><p>&nbsp;</p>

                        <p>4/23</p>
                      </td>


                        
                      <td><p>
                        </p><h1><strong><span style="color: rgb(255, 0, 0); font-weight: bold;">Learning
                        in High Dimensions</span> (1 lecture) </strong> </h1>

                        <h1><strong>Lecture 23: Dimensionality reduction
                        </strong>
                        <!-- [<a href="myslides/week1_1.pdf">slides</a>] --><br />
                        </h1>
                        <ul>
                          <li>Curse of Dimensionality</li>
                          <li>Feature Selection</li>
                          <li>Principal Component Analysis (PCA) </li>
                        </ul>

                        <p><span style="color: rgb(0, 0, 255); font-weight: bold;"><u>Readings</u>:</span>
                        </p>
                        <ul>
                          <li><a href="http://www.snl.salk.edu/%7Eshlens/pub/notes/pca.pdf">Shlens'
                            PCA tutorial</a> </li>
                          <li>(Optional) Murphy, 12.2-12.5</li>
                        </ul>
                      </td>


 <tr >
                      <td height="0" valign="top"><p>&nbsp;</p>

                        <p>4/28</p>
                      </td>



<td><p>

                        </p><h1><strong><span style="color: rgb(255, 0, 0); font-weight: bold;">
Semi-supervised learning</span> (1 lecture)
                        </strong> </h1>
<h1><strong>Lecture 22: Semi-Supervised Learning
                        </strong>
                        <!-- [<a href="myslides/week1_1.pdf">slides</a>] --><br />
                        </h1>
                        <ul>
                          <li>Mixture Models</li>
                          <li>Graph Regularization</li>
                          <li>Co-training</li>
                        </ul>

                        <p><span style="color: rgb(0, 0, 255); font-weight: bold;"><u>Readings</u>:</span>
                        </p>
                        <ul>
                          <li><a href="http://www.ark.cs.cmu.edu/LS2/images/3/32/BlumMitchell98.pdf">Combining
                            Labeled and Unlabeled Data with Co-Training</a> by
                            Michell &amp; Blum </li>
                        </ul>
                      </td>

  </tr>

                    </tr>
                    <tr>
                      <td height="0" valign="top">

                  
                  <p>4/30<br />
                  <br />
5/5</p>


                        
                  <p>5/7</p>


                        
                  <p><b>5/19</b></p>

                      </td>
                      <td><h1>
                        <div style="padding: 1px; width: 690px; background-color: rgb(252, 252, 173);">
                        <strong>Project Presentations I<br />
                        </strong></div>
                         <!-- [<a href="myslides/week1_1.pdf">slides</a>] -->
                        </h1>

                        <h1>
                        <div style="padding: 1px; width: 690px; background-color: rgb(252, 252, 173);">
                        <strong>Last Lecture and Final Exam Overview <br />
                        </strong></div>
                         <!-- [<a href="myslides/week1_1.pdf">slides</a>] -->
                        </h1>

                        <h1>
                        <div style="padding: 1px; width: 690px; background-color: rgb(252, 252, 173);">
                        <strong>Project Presentations II<br />
                        </strong></div>
                         <!-- [<a href="myslides/week1_1.pdf">slides</a>] -->
                        </h1>

                        <h1>
                        <div style="padding: 1px; width: 690px; background-color: rgb(252, 252, 173);">
                        <strong>Final Exam<br />
                        </strong></div>
                         <!-- [<a href="myslides/week1_1.pdf">slides</a>] -->
                        </h1>
                      </td>
                    </tr>
                  </tbody>
                </table>
                <br />

                <hr style="width: 100%; height: 2px;" />
                <span style="font-style: italic;">Last modified: 2014, by Leman
                Akoglu</span><br />
                <br />
                </div>
              </td>
            </tr>
          </tbody>
        </table>
      </td>
    </tr>
  </tbody>
</table>
</body></html>
